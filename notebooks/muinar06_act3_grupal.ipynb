{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ACTIVIDAD 3 (GRUPAL): REDES NEURONALES RECURRENTES (RNN)\n",
        "\n",
        "---\n",
        "\n",
        "En esta actividad grupal, vamos a utilizar las RNN para identificar la intensidad de odio en mensajes escritos en las redes sociales. Esta actividad forma parte del proyecto transversal del Máster, ¡así que seguro que ya estás familiarizado/a con el dataset!\n",
        "\n",
        "Por sencillez, se adjunta un documento csv que es una versión resumida del dataset original del proyecto transversal. En este dataset, solo se incluyen los mensajes que han sido catalogados como contenedores de odio. También se han eliminado las features innecesarias para este trabajo simplemente para hacer todo el proceso más rápido computacionalmente hablando.\n",
        "\n",
        "El dataset a utilizar consiste en 12280 mensajes de odio. Los mensajes pueden pertenecer a cinco categorias (numeradadas del 1 al 5), siendo 1 la menor intensidadd y 5 la mayor intensidad de odio del mensaje. El dataset ya ha sido procesado para eliminar ciertos caracteres que podrían darte problemas y para utilizar una codificación más amigable."
      ],
      "metadata": {
        "id": "KVfeIL4OiTON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empezamos, como siempre, incluyendo todo lo que vamos a necesitar. Según lo que vayas a hacer, es posible que necesites incluir alguna librería extra."
      ],
      "metadata": {
        "id": "Qeb4aqTDjtZu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2gvn3m1cgZ7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a3887a-3542-4e99-8c45-353e8b2443f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el dataset. Lo encontrarás adjunto a este Notebook, al descargar la actividad del Campus Virtual."
      ],
      "metadata": {
        "id": "_xvORCesj6S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Leemos el fichero\n",
        "data = pd.read_csv('dataset.csv', sep=\";\", encoding='utf-8')\n",
        "# Mostramos las 5 primeras filas\n",
        "data.head()"
      ],
      "metadata": {
        "id": "sI11OfLxgh9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cambiamos el nombre de las features para hacer su manejo más amigable:"
      ],
      "metadata": {
        "id": "WYJWQvEqkB2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.rename(columns={'CONTENIDO A ANALIZAR': 'mensaje', 'INTENSIDAD': 'intensidad'})\n",
        "data.head()"
      ],
      "metadata": {
        "id": "nTawYDb6NmmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos información básica para saber a qué nos enfrentamos:"
      ],
      "metadata": {
        "id": "rSrgxwcHkIpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "D8sOe2k-jb9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "r70C7LsWnOTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos ahora la distribución de mensajes de acuerdo a su intensidad. Verás rápidamente que estamos en problemas: hay un gran desbalanceo."
      ],
      "metadata": {
        "id": "uDpDVmNAkNDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.pie(data['intensidad'].value_counts().values,\n",
        "        labels = data['intensidad'].value_counts().index,\n",
        "        autopct='%1.1f%%')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HATeYyBcnTry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para hacer todo más sencillo y evitar asociaciones indeseadas, vamos a trabajar solo con minúsculas. Además, quitamos los signos de puntuación de los mensajes"
      ],
      "metadata": {
        "id": "fTYQhBW_keNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertimos a minúsculas\n",
        "data['mensaje'] = data['mensaje'].map(lambda x: x.lower())\n",
        "\n",
        "# Función para eliminar signos de puntuación\n",
        "def eliminar_signos(texto):\n",
        "    return re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", texto)\n",
        "\n",
        "# Aplicamos la función al dataset\n",
        "data['mensaje'] = data['mensaje'].map(eliminar_signos)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "q-uS0vz0pxW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminamos las \"stopwords\" o palabras vacías. Las stopwords son palabras comunes que, en general, no contienen información relevante para el análisis de texto y suelen ser filtradas antes de procesar los datos. Ejemplos típicos de stopwords en español son: \"el\", \"la\", \"los\", \"las\", \"un\", \"una\", \"de\", \"y\", \"o\", etc."
      ],
      "metadata": {
        "id": "MjXltaASlHiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos las stopwords en español\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Definimos una función para eliminar stopwords y lematizar\n",
        "def filtrar_palabras(texto):\n",
        "    palabras_filtradas = []\n",
        "    for palabra in texto.split():\n",
        "        if palabra not in stop_words:\n",
        "            palabra_lema = lemmatizer.lemmatize(palabra)\n",
        "            palabras_filtradas.append(palabra_lema)\n",
        "    return ' '.join(palabras_filtradas)\n",
        "\n",
        "# Aplicamos la función\n",
        "data['mensaje'] = data['mensaje'].apply(filtrar_palabras)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "O4eVyt4eq2dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos los conjuntos de entrenamiento y de test:"
      ],
      "metadata": {
        "id": "ucxI_eXQlZ_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['mensaje']\n",
        "y = data['intensidad']\n",
        "\n",
        "# Dividimos el dataset en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convertimos a numpy arrays\n",
        "X_train = X_train.values\n",
        "X_test = X_test.values\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values\n",
        "\n",
        "# Creamos datasets de TensorFlow\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
      ],
      "metadata": {
        "id": "FZxfXScRYVts"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos un mensaje y su intensidad, simplemente para ver que todo va bien:"
      ],
      "metadata": {
        "id": "NTEHtlXpnCOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for men, inte in train_dataset.take(1):\n",
        "  print('mensaje: ', men.numpy())\n",
        "  print('nivel de odio: ', inte.numpy())"
      ],
      "metadata": {
        "id": "WWz4VaBCYsjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mezclamos los datos de forma aleatoria:"
      ],
      "metadata": {
        "id": "iGisxv3GnZvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "ecZdI0CFYz1Y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "wOJwWckbY9ns"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for men, inte in train_dataset.take(1):\n",
        "  print('mensaje: ', men.numpy()[:3])\n",
        "  print()\n",
        "  print('nivel de odio: ', inte.numpy()[:3])"
      ],
      "metadata": {
        "id": "mGW0FVeyY_fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos y configuramos un codificador de texto utilizando la capa TextVectorization"
      ],
      "metadata": {
        "id": "dwTWx49snn-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Empezamos definiendo el tamaño del vocabulario\n",
        "tamaño_vocabulario = 1000\n",
        "\n",
        "# Creamos una capa de vectorización de texto\n",
        "vectorizador = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=tamaño_vocabulario\n",
        ")\n",
        "\n",
        "# Se extraen solo los textos del conjunto de datos de entrenamiento\n",
        "def extraer_texto(texto, etiqueta):\n",
        "    return texto\n",
        "\n",
        "# Adaptamos el vectorizador al texto del conjunto de datos de entrenamiento\n",
        "dataset_texto = train_dataset.map(extraer_texto)\n",
        "vectorizador.adapt(dataset_texto)"
      ],
      "metadata": {
        "id": "V8W8LqSRZCnl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos el vocabulario aprendido por la capa de vectorización\n",
        "vocabulario = np.array(vectorizador.get_vocabulary())\n",
        "\n",
        "# Mostramos las primeras 20 palabras del vocabulario\n",
        "primeras_20_palabras = vocabulario[:20]\n",
        "primeras_20_palabras"
      ],
      "metadata": {
        "id": "ps6hMfc3ZII1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio\n",
        "\n",
        "Construye una RNN que pueda detectar el nivel de odio de un mensaje.\n",
        "\n",
        "**Tenéis plena liberta para realizar la red y para hacer cualquier modificación a los datos.**"
      ],
      "metadata": {
        "id": "n1ztdBqGo6xb"
      }
    }
  ]
}